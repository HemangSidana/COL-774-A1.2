{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87595, 1184)\n",
      "[1. 0. 0. ... 0. 0. 4.]\n"
     ]
    }
   ],
   "source": [
    "train = np.loadtxt('Assignment1.2/train1.csv', delimiter=',', skiprows=1)\n",
    "print(train.shape)\n",
    "print(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87595, 1184) (87595,)\n",
      "[1. 2. 3. 4.] [16131  1032 25406 45026]\n"
     ]
    }
   ],
   "source": [
    "# append a column of ones to the train data and remove the last column as class_label\n",
    "X = train[:, :-1]\n",
    "one = np.ones((X.shape[0], 1))\n",
    "X= np.hstack((one,X))\n",
    "Y = train[:, -1]\n",
    "print(X.shape, Y.shape)\n",
    "unique, counts = np.unique(Y, return_counts=True)\n",
    "print(unique, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1184, 4)\n"
     ]
    }
   ],
   "source": [
    "W = np.zeros((X.shape[1], len(unique)), dtype=np.float64)\n",
    "print(W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(W,x,j):\n",
    "    z = np.dot(x, W)\n",
    "    # z -= np.max(z)    # TODO: for numerical stability (might add this)\n",
    "    sm = (np.exp(z[j]) / np.sum(np.exp(z)))\n",
    "    return sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87595 4\n",
      "2.373927212376198e-05\n"
     ]
    }
   ],
   "source": [
    "def g(W,x,j):\n",
    "    z = np.dot(x, W)\n",
    "    # z -= np.max(z)    # TODO: for numerical stability (might add this)\n",
    "    sm = (np.exp(z[j]) / np.sum(np.exp(z)))\n",
    "    return sm\n",
    "\n",
    "def loss(X,Y,W,counts):\n",
    "    # X is n * m\n",
    "    # Y is n * 1\n",
    "    # W is m * k\n",
    "    n = X.shape[0]\n",
    "    k = W.shape[1]\n",
    "    print(n,k)\n",
    "    loss = 0\n",
    "    for i in range(n):\n",
    "        x = X[i]\n",
    "        y = Y[i]\n",
    "        for j in range (k):\n",
    "            if (y == j):\n",
    "                loss += (np.log(g(W, x, j)) / counts[j-1])\n",
    "    loss /= -2*n\n",
    "    return loss\n",
    "\n",
    "print(loss(X,Y,W,counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(W, X):\n",
    "    # print(W.shape, X.shape)\n",
    "    z = np.dot(X, W)\n",
    "    # print(z.shape)\n",
    "    # print(z[:2])\n",
    "    # z -= np.max(z, axis=1, keepdims=True)  # For numerical stability\n",
    "    exp_scores = np.exp(z)\n",
    "    # print(exp_scores.shape)\n",
    "    # print(exp_scores[:2])\n",
    "    softmax = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    return softmax\n",
    "\n",
    "def loss(X, Y, W, counts):\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    # Calculate softmax for all classes\n",
    "    softmax = g(W, X)\n",
    "    # print(softmax.shape)\n",
    "    # print(softmax[:2])\n",
    "    \n",
    "    # Convert Y to integer type if necessary\n",
    "    Y = Y.flatten().astype(int) -1  # 0-indexing\n",
    "    \n",
    "    # Extract the softmax probabilities for the correct class labels\n",
    "    correct_class_probabilities = softmax[np.arange(n), Y]\n",
    "    # print(correct_class_probabilities.shape)\n",
    "    # print(correct_class_probabilities[:2])\n",
    "    \n",
    "    # Logarithm of the probabilities\n",
    "    log_probs = np.log(correct_class_probabilities)\n",
    "    \n",
    "    # Calculate the weighted loss (using counts for balancing, if required)\n",
    "    weights = 1 / counts[Y]\n",
    "    # print(weights.shape)\n",
    "    # print(weights[:2])\n",
    "    loss = -np.sum(log_probs * weights) / n\n",
    "    loss /= 2\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# print(loss(X, Y, W, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In g fxn\n",
      "(87595, 4)\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "(87595, 4)\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "(87595, 4)\n",
      "[[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "exiting g fxn\n",
      "[[ 1.09911699e-23 -5.50796240e-23  5.89166316e-24  4.49301203e-24]\n",
      " [-1.37509156e-09 -1.13442682e-10 -1.30177216e-09  2.79030641e-09]\n",
      " [-1.06401690e-09  2.15899451e-09 -2.21224561e-09  1.11726800e-09]]\n",
      "In loss fxn\n",
      "[[ 1.09911699e-23 -5.50796240e-23  5.89166316e-24  4.49301203e-24]\n",
      " [-1.37509156e-09 -1.13442682e-10 -1.30177216e-09  2.79030641e-09]\n",
      " [-1.06401690e-09  2.15899451e-09 -2.21224561e-09  1.11726800e-09]]\n",
      "In g fxn\n",
      "(87595, 4)\n",
      "[[  66989.49334257   39615.46910484   91918.18354689 -198523.1459943 ]\n",
      " [ 109598.3013608    64820.83301065  150392.6856745  -324811.82004596]\n",
      " [  28432.61681618   16813.02775578   39011.80659712  -84257.45116909]]\n",
      "(87595, 4)\n",
      "[[inf inf inf  0.]\n",
      " [inf inf inf  0.]\n",
      " [inf inf inf  0.]]\n",
      "(87595, 4)\n",
      "[[nan nan nan  0.]\n",
      " [nan nan nan  0.]\n",
      " [nan nan nan  0.]]\n",
      "exiting g fxn\n",
      "(87595, 4)\n",
      "[[nan nan nan  0.]\n",
      " [nan nan nan  0.]\n",
      " [nan nan nan  0.]]\n",
      "(87595,)\n",
      "[ 0. nan  0.]\n",
      "exitint loss fxn\n",
      "Iteration 0/1: Loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_80498/3498699899.py:7: RuntimeWarning: overflow encountered in exp\n",
      "  exp_scores = np.exp(z)\n",
      "/tmp/ipykernel_80498/3498699899.py:10: RuntimeWarning: invalid value encountered in divide\n",
      "  softmax = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
      "/tmp/ipykernel_80498/3498699899.py:27: RuntimeWarning: divide by zero encountered in log\n",
      "  log_probs = np.log(correct_class_probabilities)\n"
     ]
    }
   ],
   "source": [
    "def g(W, X):\n",
    "    print(\"In g fxn\")\n",
    "    z = np.dot(X, W)\n",
    "    z -= np.max(z, axis=1, keepdims=True)  # For numerical stability\n",
    "    print(z.shape)\n",
    "    print(z[:3])\n",
    "    exp_scores = np.exp(z)\n",
    "    print(exp_scores.shape)\n",
    "    print(exp_scores[:3])\n",
    "    softmax = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    print(softmax.shape)\n",
    "    print(softmax[:3])\n",
    "    print(\"exiting g fxn\")\n",
    "    return softmax\n",
    "\n",
    "def loss(X, Y, W, counts):\n",
    "    print(\"In loss fxn\")\n",
    "    print(W[:3])\n",
    "    n = X.shape[0]\n",
    "    softmax = g(W, X)\n",
    "    print(softmax.shape)\n",
    "    print(softmax[:3])\n",
    "    Y = Y.flatten().astype(int) - 1  # Convert to 0-indexed\n",
    "    correct_class_probabilities = softmax[np.arange(n), Y]\n",
    "    print(correct_class_probabilities.shape)\n",
    "    print(correct_class_probabilities[:3])\n",
    "    log_probs = np.log(correct_class_probabilities)\n",
    "    weights = 1 / counts[Y]\n",
    "    loss = -np.sum(log_probs * weights) / n\n",
    "    loss /= 2  # If you want to add this factor\n",
    "    print(\"exitint loss fxn\")\n",
    "    return loss\n",
    "\n",
    "def compute_gradient(X, Y, W, counts):\n",
    "    n, m = X.shape\n",
    "    k = W.shape[1]\n",
    "    softmax = g(W, X)\n",
    "    # print(\"Softmax shape: \", softmax.shape)\n",
    "    # print(softmax[:2])\n",
    "    Y = Y.flatten().astype(int) - 1  # Convert to 0-indexed\n",
    "    \n",
    "    # Create a one-hot encoding of Y\n",
    "    Y_one_hot = np.zeros((n, k))\n",
    "    Y_one_hot[np.arange(n), Y] = 1\n",
    "    # print(\"Y one hot shape: \", Y_one_hot.shape)\n",
    "    # print(Y_one_hot[:2])\n",
    "    \n",
    "    # Compute the gradient\n",
    "    weights = 1 / counts[Y]\n",
    "    # print(\"Weights shape: \", weights.shape)\n",
    "    # print(weights[:2])\n",
    "    gradient = np.dot(X.T, (softmax - Y_one_hot) * weights[:, np.newaxis]) / (2*n)\n",
    "    \n",
    "    return gradient\n",
    "\n",
    "def gradient_descent(X, Y, W, counts, learning_rate, iterations):\n",
    "    for i in range(iterations):\n",
    "        gradient = compute_gradient(X, Y, W, counts)\n",
    "        # print(gradient.shape)\n",
    "        # print(gradient[:4])\n",
    "        W -= learning_rate * gradient  # Update rule\n",
    "        print(W[:3])\n",
    "        \n",
    "        if i % 2 == 0:\n",
    "            # Print the loss every 100 iterations\n",
    "            current_loss = loss(X, Y, W, counts)\n",
    "            print(f\"Iteration {i}/{iterations}: Loss = {current_loss}\")\n",
    "    \n",
    "    return W\n",
    "\n",
    "X = train[:, :-1]\n",
    "one = np.ones((X.shape[0], 1))\n",
    "X= np.hstack((one,X))\n",
    "Y = train[:, -1]\n",
    "unique, counts = np.unique(Y, return_counts=True)\n",
    "W = np.zeros((X.shape[1], len(unique)), dtype=np.float64)\n",
    "\n",
    "W = gradient_descent(X, Y, W, counts, learning_rate=0.01, iterations=1)\n",
    "# print(W[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss = nan\n",
      "Iteration 1: Loss = nan\n",
      "Iteration 2: Loss = nan\n",
      "Iteration 3: Loss = nan\n",
      "Iteration 4: Loss = nan\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index -5 is out of bounds for axis 1 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m: Loss = \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (i, loss))\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m W\n\u001b[0;32m---> 29\u001b[0m W \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcounts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# print(W)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[31], line 13\u001b[0m, in \u001b[0;36mgradient_descent\u001b[0;34m(X, Y, W, counts, learning_rate, num_iters)\u001b[0m\n\u001b[1;32m     10\u001b[0m Y \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# 0-indexing\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Extract the softmax probabilities for the correct class labels\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m correct_class_probabilities \u001b[38;5;241m=\u001b[39m \u001b[43msoftmax\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Calculate the gradient\u001b[39;00m\n\u001b[1;32m     16\u001b[0m softmax[np\u001b[38;5;241m.\u001b[39marange(n), Y] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mIndexError\u001b[0m: index -5 is out of bounds for axis 1 with size 4"
     ]
    }
   ],
   "source": [
    "def gradient_descent(X, Y, W, counts, learning_rate=0.01, num_iters=100):\n",
    "    n = X.shape[0]\n",
    "    k = W.shape[1]\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate softmax for all classes\n",
    "        softmax = g(W, X)\n",
    "        \n",
    "        # Convert Y to integer type if necessary\n",
    "        Y = Y.flatten().astype(int) - 1  # 0-indexing\n",
    "        \n",
    "        # Extract the softmax probabilities for the correct class labels\n",
    "        correct_class_probabilities = softmax[np.arange(n), Y]\n",
    "        \n",
    "        # Calculate the gradient\n",
    "        softmax[np.arange(n), Y] -= 1\n",
    "        dW = np.dot(X.T, softmax)\n",
    "        \n",
    "        # Update the weights\n",
    "        W -= learning_rate * dW\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = -np.sum(np.log(correct_class_probabilities)) / n\n",
    "        loss /= 2\n",
    "        print('Iteration %d: Loss = %f' % (i, loss))\n",
    "        \n",
    "    return W\n",
    "\n",
    "W = gradient_descent(X, Y, W, counts, learning_rate=0.01, num_iters=100)\n",
    "# print(W)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

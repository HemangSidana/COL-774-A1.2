{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(data):\n",
    "    X = data[:, :-1]\n",
    "    one = np.ones((X.shape[0], 1))\n",
    "    X= np.hstack((one,X))\n",
    "    X= X.astype(np.float64)\n",
    "    Y = data[:, -1]\n",
    "    Y = Y.astype(int)-1\n",
    "    unique, counts = np.unique(Y, return_counts=True)\n",
    "    W =np.zeros((X.shape[1], len(unique)), dtype=np.float64)\n",
    "    return X,Y,W,counts\n",
    "\n",
    "def g(W,x,j):\n",
    "    z = np.dot(x,W)\n",
    "    sm= softmax(z)\n",
    "    return sm[j]\n",
    "\n",
    "def loss(X,Y,W,counts):\n",
    "    n = X.shape[0]\n",
    "    Z = X @ W\n",
    "    softmax_probs = softmax(Z, axis=1)\n",
    "    indices = (np.arange(n), Y)\n",
    "    correct_class_probs = softmax_probs[indices]\n",
    "    scaled_probs = np.log(correct_class_probs) / counts[Y]\n",
    "    loss_value = -np.mean(scaled_probs) / 2\n",
    "    return loss_value\n",
    "\n",
    "def compute_gradient(X, Y, W, counts):\n",
    "    n, m = X.shape\n",
    "    k = W.shape[1]\n",
    "    z = X @ W  \n",
    "    softmax_probs = softmax(z, axis=1) \n",
    "    indices = (np.arange(n), Y)\n",
    "    Y_one_hot = np.zeros((n, k))\n",
    "    Y_one_hot[indices] = 1\n",
    "    grad_W = X.T @ ((softmax_probs - Y_one_hot) / counts[Y][:, np.newaxis]) / (2 * n) \n",
    "    return grad_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "[1e-09, 1e-09, 1e-09]\n",
      "[4.811658557148091e-12, 4.811658557147264e-12, 4.811658557148091e-12]\n",
      "[2.9634634330326673e-12, 2.9634634330326673e-12, 5.926926866066989e-12]\n",
      "[3.650348591704111e-12, 3.650348591704111e-12, 3.650348591704938e-12]\n",
      "[4.49644314568763e-12, 4.4964431456884575e-12, 4.4964431456884575e-12]\n"
     ]
    }
   ],
   "source": [
    "def compute_n_partb(X, Y, W, gradient, n0, counts):\n",
    "    nl = 0.0\n",
    "    nh = n0\n",
    "    # print(nh)\n",
    "    while loss(X, Y, W, counts) > loss(X, Y, W - nh*gradient, counts):\n",
    "        nh *= 2\n",
    "        # print(nh)\n",
    "    while loss(X, Y, W, counts) < loss(X, Y, W - nh*gradient, counts):\n",
    "        nh /= 2\n",
    "        # print(nh)\n",
    "    nh *= 2\n",
    "    # print(\"finally \", nh)\n",
    "    for _ in range(20):\n",
    "        n1 = (2*nl + nh)/3\n",
    "        n2 = (nl + 2*nh)/3\n",
    "        if loss(X, Y, W - n1*gradient, counts) > loss(X, Y, W - n2*gradient, counts):\n",
    "            nl = n1\n",
    "        else:\n",
    "            nh = n2\n",
    "    return nh-nl,(nl+nh)/2\n",
    "\n",
    "def gradient_descent3_partb(X, Y, W, counts, n0, epochs, batch_size):\n",
    "    n = X.shape[0]\n",
    "    print(np.ceil(n/batch_size))\n",
    "    # create a of size ceil(n/batch_size)\n",
    "    n_with_epochs = [n0]*int(np.ceil(n/batch_size))\n",
    "    # n_with_epochs = [n0*np.ceil(n/batch_size)]\n",
    "    # print(n_with_epochs)\n",
    "    # print(n_with_epochs)\n",
    "    # print(n_with_epochs)\n",
    "    for _ in range(epochs):\n",
    "        print(n_with_epochs)\n",
    "        batch_num = 0\n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(start + batch_size, n)\n",
    "            X_batch = X[start:end]\n",
    "            Y_batch = Y[start:end]\n",
    "            batch_loss = loss(X_batch, Y_batch, W, counts)\n",
    "            # print(f\"Epoch{i+1}, Batch{1+int(start/batch_size)}, Loss{batch_loss}\")\n",
    "\n",
    "            gradient = compute_gradient(X_batch, Y_batch, W, counts)\n",
    "            next_n0,learning_rate= compute_n_partb(X_batch, Y_batch, W, gradient, n_with_epochs[batch_num], counts)\n",
    "            n_with_epochs[batch_num] = next_n0\n",
    "            W -= learning_rate * gradient\n",
    "            batch_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87595, 1184)\n"
     ]
    }
   ],
   "source": [
    "train = np.loadtxt(\"Assignment1.2/train1.csv\", delimiter=\",\", skiprows=1)\n",
    "test = np.loadtxt(\"Assignment1.2/test1.csv\", delimiter=\",\", skiprows=1)\n",
    "actual_pred = np.loadtxt(\"Assignment1.2/test_pred1.csv\", delimiter=\",\", skiprows=1)\n",
    "X,Y,W,counts = generate_data(train)\n",
    "X_test = test\n",
    "one = np.ones((X_test.shape[0], 1))\n",
    "X_test= np.hstack((one,X_test))\n",
    "X_test= X_test.astype(np.float64)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "[3.0000000000000004e-09]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m X,Y,W,counts \u001b[38;5;241m=\u001b[39m generate_data(train)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mgradient_descent3_partb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcounts\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1e-9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m40000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 38\u001b[0m, in \u001b[0;36mgradient_descent3_partb\u001b[0;34m(X, Y, W, counts, n0, epochs, batch_size)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# print(f\"Epoch{i+1}, Batch{1+int(start/batch_size)}, Loss{batch_loss}\")\u001b[39;00m\n\u001b[1;32m     37\u001b[0m gradient \u001b[38;5;241m=\u001b[39m compute_gradient(X_batch, Y_batch, W, counts)\n\u001b[0;32m---> 38\u001b[0m next_n0,learning_rate\u001b[38;5;241m=\u001b[39m compute_n_partb(X_batch, Y_batch, W, gradient, \u001b[43mn_with_epochs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_num\u001b[49m\u001b[43m]\u001b[49m, counts)\n\u001b[1;32m     39\u001b[0m n_with_epochs[batch_num] \u001b[38;5;241m=\u001b[39m next_n0\n\u001b[1;32m     40\u001b[0m W \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m gradient\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "X,Y,W,counts = generate_data(train)\n",
    "gradient_descent3_partb(X,Y,W,counts,1e-9,5,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1184, 4)\n",
      "[[-2.26730609e-14 -1.46447428e-14 -2.89498998e-14  6.62677034e-14]\n",
      " [-1.04586825e-14 -2.71537298e-15 -1.06120846e-14  2.37861401e-14]\n",
      " [-9.54991900e-15  1.99264410e-14 -1.94893588e-14  9.11283682e-15]\n",
      " [-1.51548296e-14  1.80077189e-14 -1.33333440e-14  1.04804547e-14]\n",
      " [-4.46943354e-15 -3.29986926e-14 -3.12370314e-15  4.05918292e-14]]\n"
     ]
    }
   ],
   "source": [
    "print(W.shape)\n",
    "print(W[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58397, 4)\n",
      "[[0.18446566 0.3616535  0.27125652 0.18262433]\n",
      " [0.16118022 0.40561638 0.27451333 0.15869007]\n",
      " [0.16176885 0.40483075 0.27392487 0.15947553]\n",
      " [0.17694939 0.37690955 0.27044707 0.17569399]\n",
      " [0.16216458 0.40432129 0.27349919 0.16001495]]\n",
      "(58397,)\n",
      "[2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "Z = X_test @ W\n",
    "softmax_probs = softmax(Z, axis=1)\n",
    "output_model_pred = np.argmax(softmax_probs, axis=1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the softmax_probs in a csv file, with each row as comma separated value as probability of each class\n",
    "np.savetxt(\"modelpredictionsb.csv\", softmax_probs, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11018   677 16685 30017]\n",
      "[3 3 3 3 3 3 3 3 3 3]\n",
      "-5.527642266228281\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def loss_test_set(test, actual_pred, W):\n",
    "    X_test = test\n",
    "    one = np.ones((X_test.shape[0], 1))\n",
    "    X_test= np.hstack((one,X_test))\n",
    "    X_test= X_test.astype(np.float64)\n",
    "    unique, counts = np.unique(actual_pred, return_counts=True)\n",
    "    print(counts)\n",
    "    Z = X_test @ W\n",
    "    softmax_probs = softmax(Z, axis=1)\n",
    "    indices = np.argmax(softmax_probs, axis=1) +  1\n",
    "    print(indices[:10])\n",
    "    loss = 0\n",
    "    for i in range(len(indices)):\n",
    "        j = int(actual_pred[i] - 1)\n",
    "        loss += np.log(softmax_probs[i][j]) / counts[j]\n",
    "    return loss\n",
    "\n",
    "print(loss_test_set(test, actual_pred, W))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

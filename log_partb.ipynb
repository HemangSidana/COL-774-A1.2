{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(data):\n",
    "    X = data[:, :-1]\n",
    "    # one = np.ones((X.shape[0], 1))\n",
    "    # X= np.hstack((one,X))\n",
    "    X= X.astype(np.float64)\n",
    "    Y = data[:, -1]\n",
    "    Y = Y.astype(int)-1\n",
    "    unique, counts = np.unique(Y, return_counts=True)\n",
    "    W =np.zeros((X.shape[1]+1, len(unique)), dtype=np.float64)\n",
    "    return X,Y,W,counts\n",
    "\n",
    "def g(W,x,j):\n",
    "    z = np.dot(x,W)\n",
    "    sm= softmax(z)\n",
    "    return sm[j]\n",
    "\n",
    "def loss(X,Y,W,counts):\n",
    "    n = X.shape[0]\n",
    "    Z = X @ W\n",
    "    softmax_probs = softmax(Z, axis=1)\n",
    "    indices = (np.arange(n), Y)\n",
    "    correct_class_probs = softmax_probs[indices]\n",
    "    scaled_probs = np.log(correct_class_probs) / counts[Y]\n",
    "    loss_value = -np.mean(scaled_probs) / 2\n",
    "    return loss_value\n",
    "\n",
    "def compute_gradient(X, Y, W, counts):\n",
    "    n, m = X.shape\n",
    "    k = W.shape[1]\n",
    "    z = X @ W  \n",
    "    softmax_probs = softmax(z, axis=1) \n",
    "    indices = (np.arange(n), Y)\n",
    "    Y_one_hot = np.zeros((n, k))\n",
    "    Y_one_hot[indices] = 1\n",
    "    grad_W = X.T @ ((softmax_probs - Y_one_hot) / counts[Y][:, np.newaxis]) / (2 * n) \n",
    "    return grad_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_n_partb(X, Y, W, gradient, n0, counts):\n",
    "#     nl = 0.0\n",
    "#     nh = n0\n",
    "#     while loss(X, Y, W, counts) > loss(X, Y, W - nh*gradient, counts):\n",
    "#         nh *= 2\n",
    "#     while loss(X, Y, W, counts) < loss(X, Y, W - nh*gradient, counts):\n",
    "#         nh /= 2\n",
    "#     nh *= 2\n",
    "#     for _ in range(20):\n",
    "#         n1 = (2*nl + nh)/3\n",
    "#         n2 = (nl + 2*nh)/3\n",
    "#         if loss(X, Y, W - n1*gradient, counts) > loss(X, Y, W - n2*gradient, counts):\n",
    "#             nl = n1\n",
    "#         else:\n",
    "#             nh = n2\n",
    "#     return nh-nl,(nl+nh)/2\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# def compute_n_partb(X, Y, W, gradient, n0, counts):\n",
    "#     nl = 0.0\n",
    "#     nh = n0\n",
    "\n",
    "#     # Arrays to store n values and corresponding loss values\n",
    "#     n_values = []\n",
    "#     loss_values = []\n",
    "\n",
    "#     # Store the initial loss for W\n",
    "#     initial_loss = loss(X, Y, W, counts)\n",
    "\n",
    "#     # Find the upper bound for n (nh)\n",
    "#     while loss(X, Y, W, counts) > loss(X, Y, W - nh * gradient, counts):\n",
    "#         nh *= 2\n",
    "#         n_values.append(nh)\n",
    "#         loss_values.append(loss(X, Y, W - nh * gradient, counts))\n",
    "\n",
    "#     # Find the correct nh value\n",
    "#     while loss(X, Y, W, counts) < loss(X, Y, W - nh * gradient, counts):\n",
    "#         nh /= 2\n",
    "#         n_values.append(nh)\n",
    "#         loss_values.append(loss(X, Y, W - nh * gradient, counts))\n",
    "\n",
    "#     nh *= 2\n",
    "#     n_values.append(nh)\n",
    "#     loss_values.append(loss(X, Y, W - nh * gradient, counts))\n",
    "\n",
    "#     # Use ternary search to refine nl and nh\n",
    "#     for _ in range(5):\n",
    "#         n1 = (2 * nl + nh) / 3\n",
    "#         n2 = (nl + 2 * nh) / 3\n",
    "\n",
    "#         loss_n1 = loss(X, Y, W - n1 * gradient, counts)\n",
    "#         loss_n2 = loss(X, Y, W - n2 * gradient, counts)\n",
    "\n",
    "#         if loss_n1 > loss_n2:\n",
    "#             nl = n1\n",
    "#         else:\n",
    "#             nh = n2\n",
    "\n",
    "#         n_values.extend([n1, n2])\n",
    "#         loss_values.extend([loss_n1, loss_n2])\n",
    "\n",
    "#     # Finally, append the midpoint loss\n",
    "#     midpoint = (nl + nh) / 2\n",
    "#     final_loss = loss(X, Y, W - midpoint * gradient, counts)\n",
    "#     n_values.append(midpoint)\n",
    "#     loss_values.append(final_loss)\n",
    "\n",
    "#     # Plotting n vs Loss with color variation\n",
    "#     plot_n_vs_loss(n_values, loss_values)\n",
    "\n",
    "#     return midpoint\n",
    "\n",
    "# def plot_n_vs_loss(n_values, loss_values):\n",
    "#     # Normalize the color range based on the index of n_values\n",
    "#     colors = np.linspace(0, 1, len(n_values))\n",
    "\n",
    "#     # Create a scatter plot with color variation\n",
    "#     scatter = plt.scatter(n_values, loss_values, c=colors, cmap='viridis', edgecolor='k')\n",
    "\n",
    "#     # Plot lines connecting the points\n",
    "#     plt.plot(n_values, loss_values, color='gray', linestyle='--')\n",
    "\n",
    "#     # Add color bar to show the mapping of iteration index to color\n",
    "#     cbar = plt.colorbar(scatter)\n",
    "#     cbar.set_label('Iteration Index')\n",
    "\n",
    "#     plt.xlabel(\"n values\")\n",
    "#     plt.ylabel(\"Loss values\")\n",
    "#     plt.title(\"n vs Loss with Color Varying by Iteration Index\")\n",
    "#     plt.show()\n",
    "\n",
    "# # Usage example: You will need to pass your own X, Y, W, gradient, n0, and counts values\n",
    "# # compute_n_partb(X, Y, W, gradient, n0, counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_n_partb(X, Y, W, gradient, n0, counts):\n",
    "    nl = 0.0\n",
    "    nh = n0\n",
    "    prev_loss= loss(X, Y, W, counts)\n",
    "    while prev_loss > loss(X, Y, W - nh*gradient, counts):\n",
    "        nh *= 2\n",
    "        prev_loss= loss(X, Y, W - nh*gradient, counts)\n",
    "    if nh>n0:\n",
    "        nl= nh/2\n",
    "    else:\n",
    "        while loss(X, Y, W, counts) < loss(X, Y, W - nh*gradient, counts):\n",
    "            nh /= 2\n",
    "        nh *= 2\n",
    "    for _ in range(5):\n",
    "        n1 = (2*nl + nh)/3\n",
    "        n2 = (nl + 2*nh)/3\n",
    "        if loss(X, Y, W - n1*gradient, counts) > loss(X, Y, W - n2*gradient, counts):\n",
    "            nl = n1\n",
    "        else:\n",
    "            nh = n2\n",
    "    return (nl+nh)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def gradient_descent3_partb(X, Y, W, counts, n0, epochs, batch_size):\n",
    "    n = X.shape[0]\n",
    "    for _ in range(epochs):\n",
    "        batch_num = 0\n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(start + batch_size, n)\n",
    "            X_batch = X[start:end]\n",
    "            Y_batch = Y[start:end]\n",
    "            batch_loss = loss(X_batch, Y_batch, W, counts)\n",
    "            print(f\"Epoch{_+1}, Batch{1+int(start/batch_size)}, Loss{batch_loss}\")\n",
    "\n",
    "            gradient = compute_gradient(X_batch, Y_batch, W, counts)\n",
    "            learning_rate= compute_n_partb(X_batch, Y_batch, W, gradient, n0, counts)\n",
    "            W -= learning_rate * gradient\n",
    "            batch_num += 1\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87595, 1183)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "train = np.loadtxt(\"Assignment1.2/train1.csv\", delimiter=\",\", skiprows=1)\n",
    "test = np.loadtxt(\"Assignment1.2/test1.csv\", delimiter=\",\", skiprows=1)\n",
    "actual_pred = np.loadtxt(\"Assignment1.2/test_pred1.csv\", delimiter=\",\", skiprows=1)\n",
    "X,Y,W,counts = generate_data(train)\n",
    "X_test = test\n",
    "one = np.ones((X_test.shape[0], 1))\n",
    "X_test= X_test.astype(np.float64)\n",
    "print(X.shape)\n",
    "scaler = StandardScaler().fit(X)\n",
    "X = scaler.transform(X)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_test= np.hstack((one,X_test))\n",
    "one = np.ones((X.shape[0],1))\n",
    "X= np.hstack((one,X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1, Batch1, Loss3.165236283166598e-05\n",
      "Epoch2, Batch1, Loss2.67407516218577e-05\n",
      "Epoch3, Batch1, Loss2.5051909632517856e-05\n",
      "Epoch4, Batch1, Loss2.4098271710054052e-05\n",
      "Epoch5, Batch1, Loss2.3469161983370397e-05\n",
      "Epoch6, Batch1, Loss2.301881617296162e-05\n",
      "Epoch7, Batch1, Loss2.267904937818342e-05\n",
      "Epoch8, Batch1, Loss2.241282676487056e-05\n",
      "Epoch9, Batch1, Loss2.219809538875155e-05\n",
      "Epoch10, Batch1, Loss2.202088318279678e-05\n",
      "Epoch11, Batch1, Loss2.1871902881557348e-05\n",
      "Epoch12, Batch1, Loss2.1744728723831587e-05\n",
      "Epoch13, Batch1, Loss2.1634764328217606e-05\n",
      "Epoch14, Batch1, Loss2.1538630385672535e-05\n",
      "Epoch15, Batch1, Loss2.1453784715634292e-05\n",
      "Epoch16, Batch1, Loss2.137827713004682e-05\n",
      "Epoch17, Batch1, Loss2.1310586045850514e-05\n",
      "Epoch18, Batch1, Loss2.124950655771541e-05\n",
      "Epoch19, Batch1, Loss2.1194071932197612e-05\n",
      "Epoch20, Batch1, Loss2.114349739485046e-05\n",
      "Epoch21, Batch1, Loss2.109713913810034e-05\n",
      "Epoch22, Batch1, Loss2.105446393836321e-05\n",
      "Epoch23, Batch1, Loss2.1015026306116578e-05\n",
      "Epoch24, Batch1, Loss2.097845107439581e-05\n",
      "Epoch25, Batch1, Loss2.0944419973002916e-05\n"
     ]
    }
   ],
   "source": [
    "W= gradient_descent3_partb(X,Y,W,counts,1e4,25,87595)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1184, 4)\n",
      "[[ 0.01737195 -0.25185358  0.03091129  0.20357035]\n",
      " [-0.03893799  0.00627864 -0.01302418  0.04568354]\n",
      " [-0.02027115  0.04835526 -0.05143434  0.02335024]\n",
      " [-0.00451498  0.0011484  -0.00345959  0.00682616]\n",
      " [ 0.0128235  -0.05190228  0.01503519  0.0240436 ]]\n"
     ]
    }
   ],
   "source": [
    "print(W.shape)\n",
    "print(W[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.06555898  0.03300401 -0.64807061  0.68062558]\n",
      " [-0.4652627   0.64420059  1.39725368 -1.57619158]\n",
      " [ 0.54387605 -0.66451501 -0.88963929  1.01027825]\n",
      " [ 0.80719547 -0.76054999 -0.19910945  0.15246397]\n",
      " [-0.7802772  -0.33592207  1.79829465 -0.68209538]]\n"
     ]
    }
   ],
   "source": [
    "Z = X_test @ W\n",
    "softmax_probs = softmax(Z, axis=1)\n",
    "print(Z[:5])\n",
    "output_model_pred = np.argmax(Z, axis=1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the softmax_probs in a csv file, with each row as comma separated value as probability of each class\n",
    "np.savetxt(\"modelpredictionsb.csv\", softmax_probs, delimiter=\",\")\n",
    "np.savetxt(\"temp.csv\", output_model_pred, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.465886286518208e-05\n"
     ]
    }
   ],
   "source": [
    "actual_pred= actual_pred.astype(int)-1\n",
    "test_unique, test_counts= np.unique(actual_pred, return_counts=True)\n",
    "print(loss(X_test, actual_pred, W, test_counts))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16131  1032 25406 45026]\n"
     ]
    }
   ],
   "source": [
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

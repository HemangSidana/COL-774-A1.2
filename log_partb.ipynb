{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(data):\n",
    "    X = data[:, :-1]\n",
    "    # one = np.ones((X.shape[0], 1))\n",
    "    # X= np.hstack((one,X))\n",
    "    X= X.astype(np.float64)\n",
    "    Y = data[:, -1]\n",
    "    Y = Y.astype(int)-1\n",
    "    unique, counts = np.unique(Y, return_counts=True)\n",
    "    W =np.zeros((X.shape[1]+1, len(unique)), dtype=np.float64)\n",
    "    return X,Y,W,counts\n",
    "\n",
    "def g(W,x,j):\n",
    "    z = np.dot(x,W)\n",
    "    sm= softmax(z)\n",
    "    return sm[j]\n",
    "\n",
    "def loss(X,Y,W,counts):\n",
    "    n = X.shape[0]\n",
    "    Z = X @ W\n",
    "    softmax_probs = softmax(Z, axis=1)\n",
    "    indices = (np.arange(n), Y)\n",
    "    correct_class_probs = softmax_probs[indices]\n",
    "    scaled_probs = np.log(correct_class_probs) / counts[Y]\n",
    "    loss_value = -np.mean(scaled_probs) / 2\n",
    "    return loss_value\n",
    "\n",
    "def compute_gradient(X, Y, W, counts):\n",
    "    n, m = X.shape\n",
    "    k = W.shape[1]\n",
    "    z = X @ W  \n",
    "    softmax_probs = softmax(z, axis=1) \n",
    "    indices = (np.arange(n), Y)\n",
    "    Y_one_hot = np.zeros((n, k))\n",
    "    Y_one_hot[indices] = 1\n",
    "    grad_W = X.T @ ((softmax_probs - Y_one_hot) / counts[Y][:, np.newaxis]) / (2 * n) \n",
    "    return grad_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_n_partb(X, Y, W, gradient, n0, counts):\n",
    "#     nl = 0.0\n",
    "#     nh = n0\n",
    "#     while loss(X, Y, W, counts) > loss(X, Y, W - nh*gradient, counts):\n",
    "#         nh *= 2\n",
    "#     while loss(X, Y, W, counts) < loss(X, Y, W - nh*gradient, counts):\n",
    "#         nh /= 2\n",
    "#     nh *= 2\n",
    "#     for _ in range(20):\n",
    "#         n1 = (2*nl + nh)/3\n",
    "#         n2 = (nl + 2*nh)/3\n",
    "#         if loss(X, Y, W - n1*gradient, counts) > loss(X, Y, W - n2*gradient, counts):\n",
    "#             nl = n1\n",
    "#         else:\n",
    "#             nh = n2\n",
    "#     return nh-nl,(nl+nh)/2\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# def compute_n_partb(X, Y, W, gradient, n0, counts):\n",
    "#     nl = 0.0\n",
    "#     nh = n0\n",
    "\n",
    "#     # Arrays to store n values and corresponding loss values\n",
    "#     n_values = []\n",
    "#     loss_values = []\n",
    "\n",
    "#     # Store the initial loss for W\n",
    "#     initial_loss = loss(X, Y, W, counts)\n",
    "\n",
    "#     # Find the upper bound for n (nh)\n",
    "#     while loss(X, Y, W, counts) > loss(X, Y, W - nh * gradient, counts):\n",
    "#         nh *= 2\n",
    "#         n_values.append(nh)\n",
    "#         loss_values.append(loss(X, Y, W - nh * gradient, counts))\n",
    "\n",
    "#     # Find the correct nh value\n",
    "#     while loss(X, Y, W, counts) < loss(X, Y, W - nh * gradient, counts):\n",
    "#         nh /= 2\n",
    "#         n_values.append(nh)\n",
    "#         loss_values.append(loss(X, Y, W - nh * gradient, counts))\n",
    "\n",
    "#     nh *= 2\n",
    "#     n_values.append(nh)\n",
    "#     loss_values.append(loss(X, Y, W - nh * gradient, counts))\n",
    "\n",
    "#     # Use ternary search to refine nl and nh\n",
    "#     for _ in range(5):\n",
    "#         n1 = (2 * nl + nh) / 3\n",
    "#         n2 = (nl + 2 * nh) / 3\n",
    "\n",
    "#         loss_n1 = loss(X, Y, W - n1 * gradient, counts)\n",
    "#         loss_n2 = loss(X, Y, W - n2 * gradient, counts)\n",
    "\n",
    "#         if loss_n1 > loss_n2:\n",
    "#             nl = n1\n",
    "#         else:\n",
    "#             nh = n2\n",
    "\n",
    "#         n_values.extend([n1, n2])\n",
    "#         loss_values.extend([loss_n1, loss_n2])\n",
    "\n",
    "#     # Finally, append the midpoint loss\n",
    "#     midpoint = (nl + nh) / 2\n",
    "#     final_loss = loss(X, Y, W - midpoint * gradient, counts)\n",
    "#     n_values.append(midpoint)\n",
    "#     loss_values.append(final_loss)\n",
    "\n",
    "#     # Plotting n vs Loss with color variation\n",
    "#     plot_n_vs_loss(n_values, loss_values)\n",
    "\n",
    "#     return midpoint\n",
    "\n",
    "# def plot_n_vs_loss(n_values, loss_values):\n",
    "#     # Normalize the color range based on the index of n_values\n",
    "#     colors = np.linspace(0, 1, len(n_values))\n",
    "\n",
    "#     # Create a scatter plot with color variation\n",
    "#     scatter = plt.scatter(n_values, loss_values, c=colors, cmap='viridis', edgecolor='k')\n",
    "\n",
    "#     # Plot lines connecting the points\n",
    "#     plt.plot(n_values, loss_values, color='gray', linestyle='--')\n",
    "\n",
    "#     # Add color bar to show the mapping of iteration index to color\n",
    "#     cbar = plt.colorbar(scatter)\n",
    "#     cbar.set_label('Iteration Index')\n",
    "\n",
    "#     plt.xlabel(\"n values\")\n",
    "#     plt.ylabel(\"Loss values\")\n",
    "#     plt.title(\"n vs Loss with Color Varying by Iteration Index\")\n",
    "#     plt.show()\n",
    "\n",
    "# # Usage example: You will need to pass your own X, Y, W, gradient, n0, and counts values\n",
    "# # compute_n_partb(X, Y, W, gradient, n0, counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_n_partb(X, Y, W, gradient, n0, counts):\n",
    "    nl = 0.0\n",
    "    nh = n0\n",
    "    prev_loss= loss(X, Y, W, counts)\n",
    "    while prev_loss > loss(X, Y, W - nh*gradient, counts):\n",
    "        prev_loss= loss(X, Y, W - nh*gradient, counts)\n",
    "        nh *= 2\n",
    "    if abs(nh-n0)<1e-9:\n",
    "        while loss(X, Y, W, counts) < loss(X, Y, W - nh*gradient, counts):\n",
    "            nh /= 2\n",
    "        nh *= 2\n",
    "    for _ in range(5):\n",
    "        n1 = (2*nl + nh)/3\n",
    "        n2 = (nl + 2*nh)/3\n",
    "        if loss(X, Y, W - n1*gradient, counts) > loss(X, Y, W - n2*gradient, counts):\n",
    "            nl = n1\n",
    "        else:\n",
    "            nh = n2\n",
    "    return (nl+nh)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def gradient_descent3_partb(X, Y, W, counts, n0, epochs, batch_size):\n",
    "    n = X.shape[0]\n",
    "    for _ in range(epochs):\n",
    "        batch_num = 0\n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(start + batch_size, n)\n",
    "            X_batch = X[start:end]\n",
    "            Y_batch = Y[start:end]\n",
    "            batch_loss = loss(X_batch, Y_batch, W, counts)\n",
    "            print(f\"Epoch{_+1}, Batch{1+int(start/batch_size)}, Loss{batch_loss}\")\n",
    "\n",
    "            gradient = compute_gradient(X_batch, Y_batch, W, counts)\n",
    "            learning_rate= compute_n_partb(X_batch, Y_batch, W, gradient, n0, counts)\n",
    "            W -= learning_rate * gradient\n",
    "            batch_num += 1\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87595, 1183)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "train = np.loadtxt(\"Assignment1.2/train1.csv\", delimiter=\",\", skiprows=1)\n",
    "test = np.loadtxt(\"Assignment1.2/test1.csv\", delimiter=\",\", skiprows=1)\n",
    "actual_pred = np.loadtxt(\"Assignment1.2/test_pred1.csv\", delimiter=\",\", skiprows=1)\n",
    "X,Y,W,counts = generate_data(train)\n",
    "X_test = test\n",
    "one = np.ones((X_test.shape[0], 1))\n",
    "X_test= X_test.astype(np.float64)\n",
    "print(X.shape)\n",
    "scaler = StandardScaler().fit(X)\n",
    "X = scaler.transform(X)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_test= np.hstack((one,X_test))\n",
    "one = np.ones((X.shape[0],1))\n",
    "X= np.hstack((one,X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1, Batch1, Loss3.165236283166598e-05\n",
      "Epoch2, Batch1, Loss2.475439726503823e-05\n",
      "Epoch3, Batch1, Loss2.291654817775092e-05\n",
      "Epoch4, Batch1, Loss2.2246727351364132e-05\n",
      "Epoch5, Batch1, Loss2.1830604666013927e-05\n",
      "Epoch6, Batch1, Loss2.1558488174050303e-05\n",
      "Epoch7, Batch1, Loss2.1353997701490082e-05\n",
      "Epoch8, Batch1, Loss2.119708134274187e-05\n",
      "Epoch9, Batch1, Loss2.1069220925874506e-05\n",
      "Epoch10, Batch1, Loss2.096638970421969e-05\n",
      "Epoch11, Batch1, Loss2.0883063529037526e-05\n",
      "Epoch12, Batch1, Loss2.081534180330254e-05\n",
      "Epoch13, Batch1, Loss2.0757484251740366e-05\n",
      "Epoch14, Batch1, Loss2.07074253472993e-05\n",
      "Epoch15, Batch1, Loss2.0663017208185433e-05\n",
      "Epoch16, Batch1, Loss2.0622706020865116e-05\n",
      "Epoch17, Batch1, Loss2.0586200475372847e-05\n",
      "Epoch18, Batch1, Loss2.055203396093484e-05\n",
      "Epoch19, Batch1, Loss2.0522096411227737e-05\n",
      "Epoch20, Batch1, Loss2.049365613490433e-05\n",
      "Epoch21, Batch1, Loss2.0468709358615234e-05\n",
      "Epoch22, Batch1, Loss2.0444087976811308e-05\n",
      "Epoch23, Batch1, Loss2.042031652875496e-05\n",
      "Epoch24, Batch1, Loss2.0395783880098907e-05\n",
      "Epoch25, Batch1, Loss2.037452197923535e-05\n"
     ]
    }
   ],
   "source": [
    "W= gradient_descent3_partb(X,Y,W,counts,1e4,25,87595)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1184, 4)\n",
      "[[ 0.02385923 -0.50396173  0.12265108  0.35745141]\n",
      " [-0.04375223  0.00505362 -0.01048819  0.0491868 ]\n",
      " [-0.03077164  0.05612169 -0.05248986  0.02713981]\n",
      " [-0.00302366 -0.00132851  0.00153493  0.00281723]\n",
      " [ 0.0300145  -0.06872048  0.01564375  0.02306223]]\n"
     ]
    }
   ],
   "source": [
    "print(W.shape)\n",
    "print(W[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.129259    0.12518396 -0.80655386  0.8106289 ]\n",
      " [-0.62405338  0.54188117  1.41891549 -1.33674328]\n",
      " [ 0.85573476 -1.22864679 -0.9545532   1.32746523]\n",
      " [ 0.82579188 -1.47940801  0.2755804   0.37803573]\n",
      " [-0.84518132 -0.80887482  2.13409953 -0.48004338]]\n"
     ]
    }
   ],
   "source": [
    "Z = X_test @ W\n",
    "softmax_probs = softmax(Z, axis=1)\n",
    "print(Z[:5])\n",
    "output_model_pred = np.argmax(Z, axis=1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the softmax_probs in a csv file, with each row as comma separated value as probability of each class\n",
    "np.savetxt(\"modelpredictionsb.csv\", softmax_probs, delimiter=\",\")\n",
    "np.savetxt(\"temp.csv\", output_model_pred, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.521534578755681e-05\n"
     ]
    }
   ],
   "source": [
    "actual_pred= actual_pred.astype(int)-1\n",
    "test_unique, test_counts= np.unique(actual_pred, return_counts=True)\n",
    "print(loss(X_test, actual_pred, W, test_counts))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16131  1032 25406 45026]\n"
     ]
    }
   ],
   "source": [
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
